{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import requests\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requesting corpus from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "response.text\n",
    "data = response.text.split('\\n')\n",
    "data = data[253:]                           # From line 253 real text starts\n",
    "data = \" \".join(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_text(data)\n",
    "print(\"The total number of unique words\",len(set(tokens)))       \n",
    "length = 50+1                       \n",
    "lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lines of size 51 in which 51th word act as label y and 50 word sequence act as X\n",
    "\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    lines.append(line)\n",
    "    if i > 100000:\n",
    "        break\n",
    "\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries for model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,SimpleRNN, GRU, Dropout,Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating tokens using text for feeding in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the token as pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating X and y to feed in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X,y = sequences[:, :-1], sequences[:,-1]      # Creating X and y for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes = vocab_size)     # changing y to categorical to feed in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word = vocab_size\n",
    "input_length = 50\n",
    "dropout_val = 0.2\n",
    "Dense_layers = 256\n",
    "RNN_layer = 128\n",
    "no_of_embeddings = 100\n",
    "max_pad_length = seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_RNN(layer = \"RNN\"):\n",
    "    if layer == \"RNN\":\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_word +1 ,  no_of_embeddings ,input_length=max_pad_length))\n",
    "        model.add(SimpleRNN(RNN_layer))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(Dense_layers, activation = 'relu'))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(vocab_size, activation ='softmax'))\n",
    "    \n",
    "    if layer == \"LSTM\":\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_word +1 ,  no_of_embeddings ,input_length=max_pad_length))\n",
    "        model.add(LSTM(RNN_layer,return_sequences=True))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(LSTM(RNN_layer))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(Dense_layers, activation = 'relu'))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(vocab_size, activation ='softmax'))\n",
    "        \n",
    "    if layer == \"GRU\":\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_word +1 ,  no_of_embeddings ,input_length=max_pad_length))\n",
    "        model.add(GRU(RNN_layer,return_sequences=True))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(GRU(RNN_layer))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(Dense_layers, activation = 'relu'))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(vocab_size, activation ='softmax'))\n",
    "        \n",
    "    if layer == \"BILSTM\":\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(num_word +1 ,  no_of_embeddings ,input_length=max_pad_length))\n",
    "        model.add(Bidirectional(LSTM(RNN_layer,return_sequences=True)))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Bidirectional(LSTM(RNN_layer)))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(Dense_layers, activation = 'relu'))\n",
    "        model.add(Dropout(dropout_val))\n",
    "        model.add(Dense(vocab_size, activation ='softmax'))\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_RNN(\"BILSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling and fitting  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer ='adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y, batch_size=256, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prdicting new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = [\"What\", \"do\", \"we\",\"know\"]\n",
    "next_words = 50\n",
    "\n",
    "for _ in range(next_words):\n",
    "    \n",
    "    texts = ' '.join(seed_text)\n",
    "    token_list = tokenizer.texts_to_sequences([texts ])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen = 50, padding =\"pre\" )\n",
    "    predicted = model.predict_classes(token_list, verbose = 0)\n",
    "    output_word = \" \"\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            seed_text.append(output_word)\n",
    "            break\n",
    "print(' '.join(seed_text))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
